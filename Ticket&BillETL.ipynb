{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51f73efd-e5f2-4a64-b71c-19da1f5994e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 --version\n",
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext.getOrCreate()\n",
    "#print(sc.pythonVer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c94b79be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e844ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\paula\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\paula\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\paula\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\paula\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\paula\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\paula\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\paula\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c672a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y pyspark\n",
    "#pip uninstall -yspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0468ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip cache purge\n",
    "\n",
    "#!pip install pyspark==3.5.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2006e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libreria para análisis EDA\n",
    "#!pip install matplotlib seaborn sweetviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999ec378-e20f-4900-ae0f-e00473f559d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in c:\\users\\paula\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\paula\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8edda788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\paula\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\paula\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\paula\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openpyxl) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0691c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b570e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = r'C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Entrega3\\TESIS_Maestria_ETL\\ETLProyect\\ArchivosEntrada\\LOCardReports\\Cash_Advance_Transaction-20240903-0921.xls'\n",
    "\n",
    "# Especificar la ruta del archivo Excel\n",
    "output_path = r'C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Entrega3\\TESIS_Maestria_ETL\\ETLProyect\\ArchivosSalida\\Concatenado2.xlsx'\n",
    "\n",
    "# Especificar la ruta del archivo Excel\n",
    "output_path3 = r'C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Entrega3\\TESIS_Maestria_ETL\\ETLProyect\\ArchivosSalida\\Concatenado3.xlsx'\n",
    "\n",
    "# Ruta de la carpeta que contiene los archivos de Entrada\n",
    "folder_path = r'C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Entrega3\\TESIS_Maestria_ETL\\ETLProyect\\ArchivosEntrada\\LOCardReports'\n",
    "folder_path_tr_bb = r'C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Entrega3\\TESIS_Maestria_ETL\\ETLProyect\\ArchivosEntrada\\LO_TR_BBReports'\n",
    "host_folder_path = r'C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Entrega3\\TESIS_Maestria_ETL\\ETLProyect\\ArchivosEntrada\\HostReports'\n",
    "Logs_path = r'C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Entrega3\\TESIS_Maestria_ETL\\ETLProyect\\ArchivosEntrada\\Logs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf579abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------SPARK Consolidado\n",
    "\n",
    "#import os\n",
    "#os.environ[\"PYSPARK_PYTHON\"] = \"python3\"  # O \"python\" si ese es tu ejecutable\n",
    "spark = SparkSession.builder.appName(\"Host_LO Comparative\").getOrCreate()\n",
    "\n",
    "#Pandas a Spark\n",
    "\n",
    "dfLO = spark.createDataFrame(df_combined)\n",
    "dfHOST = spark.createDataFrame(df_host)\n",
    "\n",
    "\n",
    "#Renombre de columnas\n",
    "dfLO = dfLO.withColumnRenamed(\"HOSTSEQ NUMBER\", \"hostseq_number\")\n",
    "dfHOST = dfHOST.withColumnRenamed(\"Seq\", \"seq\")\n",
    "\n",
    "# Filtrar el DataFrame dfLO\n",
    "#dfLO = dfLO.filter(F.col(\"STATUS\") != \"Denied\")\n",
    "dfLO = dfLO.filter(F.col(\"TRANSACTIONTYPE\") != \"Balance inquiry\")\n",
    "\n",
    "\n",
    "#Verificación\n",
    "#print('Dataframe de LiveOffice')\n",
    "#dfLO.printSchema()\n",
    "#print('Dataframe de Host')\n",
    "#dfHOST.printSchema()\n",
    "\n",
    "joined_df = dfLO.join(\n",
    "    dfHOST,\n",
    "    dfLO[\"hostseq_number\"] == dfHOST[\"seq\"],\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "\n",
    "# Agregar columna de indicador\n",
    "result_df = joined_df.withColumn(\n",
    "    \"found_in_datastream\",\n",
    "    F.when(F.col(\"seq\").isNotNull(), True).otherwise(False)\n",
    ")\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "#unmatched_df.show()\n",
    "# Convertir el DataFrame de Spark a Pandas\n",
    "pandas_df = result_df.toPandas()\n",
    "pandas_df.to_excel(output_path3, index=False, engine='openpyxl')  # Usa openpyxl como motor\n",
    "print(f\"Archivo exportado exitosamente a {output_path3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7497f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Entrega3\\TESIS_Maestria_ETL\\ETLProyect\\ArchivosEntrada\\Logs\\ATM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsATM['seqNumber'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"seqNumber\":\"(\\d+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsATM['AuthNumber'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"AuthNumber\":\"(\\d+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsATM['CardNumber'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"CardNumber\":\"(\\d+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsATM['HostIP'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"HostIP\":\"([^\"]+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsATM['Amount'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"Amount\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsATM['DispensedTotal'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"DispensedTotal\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsATM['TransactionType'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"TransactionType\":\"([^\"]+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsATM['Status'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"Status\":\"([^\"]+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsATM['JournalName'] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Entrega3\\TESIS_Maestria_ETL\\ETLProyect\\ArchivosEntrada\\Logs\\BILLBREAKING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:127: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsbb['seqNumber'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"seqNumber\":\"(\\d+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsbb['TicketData'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"TicketData\":\"([^\"]+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:129: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsbb['Amount'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"Amount\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsbb['DispensedTotal'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"DispensedTotal\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsbb['Type'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"type\"\\s*:\\s*\"([^\"]+)\"', expand=False)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsbb['BillDenom_01_1'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"BillCount_01\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsbb['BillDenom_02_5'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"BillCount_02\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:136: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsbb['BillDenom_03_10'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"BillCount_03\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsbb['BillDenom_04_20'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"BillCount_04\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:138: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsbb['BillDenom_05_50'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"BillCount_05\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:139: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsbb['BillDenom_06_100'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"BillCount_06\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:140: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsbb['Status'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"Status\":\"([^\"]+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsbb['JournalName'] = os.path.basename(ruta_subcarpeta).lower()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Entrega3\\TESIS_Maestria_ETL\\ETLProyect\\ArchivosEntrada\\Logs\\CASHADVANCE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsCA['seqNumber'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"seqNumber\":\"(\\d+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsCA['AuthNumber'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"AuthNumber\":\"(\\d+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsCA['CardNumber'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"CardNumber\":\"(\\d+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsCA['HostIP'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"HostIP\":\"([^\"]+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsCA['Amount'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"Amount\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsCA['DispensedTotal'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"DispensedTotal\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsCA['TransactionType'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"TransactionType\":\"([^\"]+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsCA['Status'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"Status\":\"([^\"]+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsCA['JournalName'] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Entrega3\\TESIS_Maestria_ETL\\ETLProyect\\ArchivosEntrada\\Logs\\TICKETREDEMPTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsTR['seqNumber'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"seqNumber\":\"(\\d+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsTR['TicketData'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"TicketData\":\"([^\"]+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsTR['Amount'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"Amount\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsTR['DispensedTotal'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"DispensedTotal\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsTR['Type'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"type\"\\s*:\\s*\"([^\"]+)\"', expand=False)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsTR['BillDenom_01_1'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"BillCount_01\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsTR['BillDenom_02_5'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"BillCount_02\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsTR['BillDenom_03_10'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"BillCount_03\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsTR['BillDenom_04_20'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"BillCount_04\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsTR['BillDenom_05_50'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"BillCount_05\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsTR['BillDenom_06_100'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"BillCount_06\":([0-9]+(?:\\.[0-9]+)?)')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsTR['Status'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"Status\":\"([^\"]+)\"').astype(str)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3243722929.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfLogsTR['JournalName'] = os.path.basename(ruta_subcarpeta).lower()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             FilteredData         seqNumber  \\\n",
      "404     00:00:31 402001 ' Posting: {\"type\":\"BillBreak\"...  0823202400000976   \n",
      "809     00:01:07 402001 ' Posting: {\"type\":\"BillBreak\"...  0823202400004898   \n",
      "1214    00:16:35 402001 ' Posting: {\"type\":\"BillBreak\"...  0823202400162040   \n",
      "1619    00:40:11 402001 ' Posting: {\"type\":\"BillBreak\"...  0823202400395160   \n",
      "2024    00:52:20 402001 ' Posting: {\"type\":\"BillBreak\"...  0823202400515980   \n",
      "...                                                   ...               ...   \n",
      "810968  23:46:10 402001 ' Posting: {\"type\":\"BillBreak\"...  0831202423454842   \n",
      "811373  23:48:12 402001 ' Posting: {\"type\":\"BillBreak\"...  0831202423475349   \n",
      "811778  23:48:41 402001 ' Posting: {\"type\":\"BillBreak\"...  0831202423482207   \n",
      "812183  23:51:15 402001 ' Posting: {\"type\":\"BillBreak\"...  0831202423504148   \n",
      "812588  23:51:58 402001 ' Posting: {\"type\":\"BillBreak\"...  0831202423512574   \n",
      "\n",
      "              TicketData Amount DispensedTotal       Type BillDenom_01_1  \\\n",
      "404      BB[2]     $20.0   20.0           20.0  BillBreak              0   \n",
      "809     BB[2]     $100.0  100.0          100.0  BillBreak              0   \n",
      "1214      BB[2]     $5.0    5.0            5.0  BillBreak              5   \n",
      "1619    BB[2]     $100.0  100.0          100.0  BillBreak              0   \n",
      "2024    BB[2]     $100.0  100.0          100.0  BillBreak              0   \n",
      "...                  ...    ...            ...        ...            ...   \n",
      "810968  BB[2]     $100.0  100.0          100.0  BillBreak              0   \n",
      "811373  BB[2]     $100.0  100.0          100.0  BillBreak              0   \n",
      "811778  BB[2]     $100.0  100.0          100.0  BillBreak              0   \n",
      "812183  BB[2]     $100.0  100.0          100.0  BillBreak              0   \n",
      "812588  BB[2]     $100.0  100.0          100.0  BillBreak              0   \n",
      "\n",
      "       BillDenom_02_5 BillDenom_03_10 BillDenom_04_20 BillDenom_05_50  \\\n",
      "404                 4               0               0               0   \n",
      "809                 0               0               5               0   \n",
      "1214                0               0               0               0   \n",
      "1619                0               0               5               0   \n",
      "2024                0               0               5               0   \n",
      "...               ...             ...             ...             ...   \n",
      "810968              0               0               5               0   \n",
      "811373              0               0               5               0   \n",
      "811778              0               0               5               0   \n",
      "812183              0               0               5               0   \n",
      "812588              0               0               5               0   \n",
      "\n",
      "       BillDenom_06_100     Status   JournalName  \n",
      "404                   0  COMPLETED  billbreaking  \n",
      "809                   0  COMPLETED  billbreaking  \n",
      "1214                  0  COMPLETED  billbreaking  \n",
      "1619                  0  COMPLETED  billbreaking  \n",
      "2024                  0  COMPLETED  billbreaking  \n",
      "...                 ...        ...           ...  \n",
      "810968                0  COMPLETED  billbreaking  \n",
      "811373                0  COMPLETED  billbreaking  \n",
      "811778                0  COMPLETED  billbreaking  \n",
      "812183                0  COMPLETED  billbreaking  \n",
      "812588                0  COMPLETED  billbreaking  \n",
      "\n",
      "[2005 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------Logs Extraction-------------------------------------------------------------------------------\n",
    "# Crear una lista para almacenar los DataFrames\n",
    "logs_dataframes = []\n",
    "logs_dataframes_tr = []\n",
    "logs_dataframes_bb = []\n",
    "\n",
    "\n",
    "column_names = [\"FilteredData\"]\n",
    "df_LogsUnified=  pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "for subcarpeta in os.listdir(Logs_path):\n",
    "    ruta_subcarpeta = os.path.join(Logs_path, subcarpeta)\n",
    "    # Verificar si es una carpeta\n",
    "    if os.path.isdir(ruta_subcarpeta) and os.path.basename(ruta_subcarpeta).lower() == 'atm':\n",
    "        print(ruta_subcarpeta)\n",
    "        for archivo in os.listdir(ruta_subcarpeta):\n",
    "            if archivo.endswith('.jrn'):  # Filtrar solo archivos con extensión .jrn\n",
    "                ruta_completa = os.path.join(ruta_subcarpeta, archivo)\n",
    "                logs_df = pd.read_csv(ruta_completa, sep=';', encoding='utf-8', names=column_names,header=None)  # Ajusta el separador si es necesario\n",
    "                logs_dataframes.append(logs_df)  \n",
    "        df_Logscompleto = pd.concat(logs_dataframes, ignore_index=True)   # Concatenar todos los DataFrames en uno solo\n",
    "\n",
    "        # Paso 1: Filtrar filas que contienen el texto \"ATM. 'Posting Transaction Result\"\n",
    "        dfLogsATM = df_Logscompleto[\n",
    "            df_Logscompleto.apply(lambda row: row.astype(str).str.contains(\"ATM.     'Posting Transaction Result \").any(), axis=1)\n",
    "        ]\n",
    "\n",
    "        # Usar expresiones regulares para extraer los valores\n",
    "        dfLogsATM['seqNumber'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"seqNumber\":\"(\\d+)\"').astype(str)\n",
    "        dfLogsATM['AuthNumber'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"AuthNumber\":\"(\\d+)\"').astype(str)\n",
    "        dfLogsATM['CardNumber'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"CardNumber\":\"(\\d+)\"').astype(str)\n",
    "        dfLogsATM['HostIP'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"HostIP\":\"([^\"]+)\"').astype(str)\n",
    "        dfLogsATM['Amount'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"Amount\":([0-9]+(?:\\.[0-9]+)?)')\n",
    "        dfLogsATM['DispensedTotal'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"DispensedTotal\":([0-9]+(?:\\.[0-9]+)?)')\n",
    "        dfLogsATM['TransactionType'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"TransactionType\":\"([^\"]+)\"').astype(str)\n",
    "        dfLogsATM['Status'] = dfLogsATM[\"FilteredData\"].str.extract(r'\"Status\":\"([^\"]+)\"').astype(str)\n",
    "        dfLogsATM['JournalName'] = (\n",
    "            dfLogsATM[\"FilteredData\"]\n",
    "            .str.extract(r'\\d+\\s+([^\\s]+)\\s+\\'Posting Transaction Result')  # Extraer el texto relevante\n",
    "            .squeeze()  # Convertir a una Serie si es un DataFrame con una sola columna\n",
    "            .str.replace(r'\\.', '', regex=True)  # Eliminar el punto\n",
    "        ).astype(str)\n",
    "\n",
    "        \n",
    "        df_LogsUnified = pd.concat([df_LogsUnified,dfLogsATM], ignore_index=True)\n",
    "\n",
    "    elif os.path.isdir(ruta_subcarpeta) and os.path.basename(ruta_subcarpeta).lower() == 'cashadvance':\n",
    "        print(ruta_subcarpeta)\n",
    "        for archivo in os.listdir(ruta_subcarpeta):\n",
    "            if archivo.endswith('.jrn'):  # Filtrar solo archivos con extensión .jrn\n",
    "                ruta_completa = os.path.join(ruta_subcarpeta, archivo)\n",
    "                logs_df = pd.read_csv(ruta_completa, sep=';', encoding='utf-8', names=column_names,header=None)  # Ajusta el separador si es necesario\n",
    "                logs_dataframes.append(logs_df)  \n",
    "        df_Logscompleto = pd.concat(logs_dataframes, ignore_index=True)   # Concatenar todos los DataFrames en uno solo\n",
    "\n",
    "        # Paso 1: Filtrar filas que contienen el texto \"ATM. 'Posting Transaction Result\"\n",
    "        dfLogsCA = df_Logscompleto[\n",
    "            df_Logscompleto.apply(lambda row: row.astype(str).str.contains(\"Cash Advance Transaction Info. 'Posting Transaction Result\").any(), axis=1)\n",
    "        ]\n",
    "        \n",
    "        # Usar expresiones regulares para extraer los valores\n",
    "        dfLogsCA['seqNumber'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"seqNumber\":\"(\\d+)\"').astype(str)\n",
    "        dfLogsCA['AuthNumber'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"AuthNumber\":\"(\\d+)\"').astype(str)\n",
    "        dfLogsCA['CardNumber'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"CardNumber\":\"(\\d+)\"').astype(str)\n",
    "        dfLogsCA['HostIP'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"HostIP\":\"([^\"]+)\"').astype(str)\n",
    "        dfLogsCA['Amount'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"Amount\":([0-9]+(?:\\.[0-9]+)?)')\n",
    "        dfLogsCA['DispensedTotal'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"DispensedTotal\":([0-9]+(?:\\.[0-9]+)?)')\n",
    "        dfLogsCA['TransactionType'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"TransactionType\":\"([^\"]+)\"').astype(str)\n",
    "        dfLogsCA['Status'] = dfLogsCA[\"FilteredData\"].str.extract(r'\"Status\":\"([^\"]+)\"').astype(str)\n",
    "        dfLogsCA['JournalName'] = (\n",
    "            dfLogsCA[\"FilteredData\"]\n",
    "            .str.extract(r'------------>\\s*(.*?)\\s*Transaction Info\\.')  # Extraer el texto relevante\n",
    "            .squeeze()  # Convertir a una Serie si es un DataFrame con una sola columna\n",
    "            .str.replace(r'\\.', '', regex=True)  # Eliminar el punto\n",
    "        ).astype(str)\n",
    "                \n",
    "        df_LogsUnified = pd.concat([df_LogsUnified,dfLogsCA], ignore_index=True)\n",
    "\n",
    "    elif os.path.isdir(ruta_subcarpeta) and os.path.basename(ruta_subcarpeta).lower() == 'ticketredemption':\n",
    "        print(ruta_subcarpeta)\n",
    "        for archivo in os.listdir(ruta_subcarpeta):\n",
    "            if archivo.endswith('.jrn'):  # Filtrar solo archivos con extensión .jrn\n",
    "                ruta_completa = os.path.join(ruta_subcarpeta, archivo)\n",
    "                logs_df_tr = pd.read_csv(ruta_completa, sep=';', encoding='utf-8', names=column_names,header=None)  # Ajusta el separador si es necesario\n",
    "                logs_dataframes_tr.append(logs_df_tr)  \n",
    "        df_Logscompleto_tr = pd.concat(logs_dataframes_tr, ignore_index=True)   # Concatenar todos los DataFrames en uno solo\n",
    "\n",
    "        # Paso 1: Filtrar filas que contienen el texto \"ATM. 'Posting Transaction Result\"\n",
    "        dfLogsTR = df_Logscompleto_tr[\n",
    "            df_Logscompleto_tr.apply(lambda row: row.astype(str).str.contains('Posting: {\"type\":\"TicketRedemption\"').any(), axis=1)\n",
    "        ]\n",
    "        \n",
    "        # Usar expresiones regulares para extraer los valores\n",
    "        dfLogsTR['seqNumber'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"seqNumber\":\"(\\d+)\"').astype(str)\n",
    "        dfLogsTR['TicketData'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"TicketData\":\"([^\"]+)\"').astype(str)\n",
    "        dfLogsTR['Amount'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"Amount\":([0-9]+(?:\\.[0-9]+)?)')\n",
    "        dfLogsTR['DispensedTotal'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"DispensedTotal\":([0-9]+(?:\\.[0-9]+)?)')\n",
    "        #dfLogsTR['type'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"TransactionType\":\"([^\"]+)\"').astype(str)\n",
    "        #dfLogsTR['Type'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"type\":\"([^\"]+)\"').astype(str)\n",
    "        dfLogsTR['Type'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"type\"\\s*:\\s*\"([^\"]+)\"', expand=False)\n",
    "        dfLogsTR['BillDenom_01_1'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"BillCount_01\":([0-9]+(?:\\.[0-9]+)?)')        \n",
    "        dfLogsTR['BillDenom_02_5'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"BillCount_02\":([0-9]+(?:\\.[0-9]+)?)')        \n",
    "        dfLogsTR['BillDenom_03_10'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"BillCount_03\":([0-9]+(?:\\.[0-9]+)?)')        \n",
    "        dfLogsTR['BillDenom_04_20'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"BillCount_04\":([0-9]+(?:\\.[0-9]+)?)')        \n",
    "        dfLogsTR['BillDenom_05_50'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"BillCount_05\":([0-9]+(?:\\.[0-9]+)?)')\n",
    "        dfLogsTR['BillDenom_06_100'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"BillCount_06\":([0-9]+(?:\\.[0-9]+)?)')\n",
    "        dfLogsTR['Status'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"Status\":\"([^\"]+)\"').astype(str)\n",
    "        dfLogsTR['JournalName'] = os.path.basename(ruta_subcarpeta).lower()\n",
    "\n",
    "    elif os.path.isdir(ruta_subcarpeta) and os.path.basename(ruta_subcarpeta).lower() == 'billbreaking':\n",
    "        print(ruta_subcarpeta)\n",
    "        for archivo in os.listdir(ruta_subcarpeta):\n",
    "            if archivo.endswith('.jrn'):  # Filtrar solo archivos con extensión .jrn\n",
    "                ruta_completa = os.path.join(ruta_subcarpeta, archivo)\n",
    "                logs_df_bb = pd.read_csv(ruta_completa, sep=';', encoding='utf-8', names=column_names,header=None)  # Ajusta el separador si es necesario\n",
    "                logs_dataframes_bb.append(logs_df_bb)  \n",
    "        df_Logscompleto_bb = pd.concat(logs_dataframes_bb, ignore_index=True)   # Concatenar todos los DataFrames en uno solo\n",
    "\n",
    "        # Paso 1: Filtrar filas que contienen el texto \"ATM. 'Posting Transaction Result\"\n",
    "        dfLogsbb = df_Logscompleto_bb[\n",
    "            df_Logscompleto_bb.apply(lambda row: row.astype(str).str.contains(' Posting: {\"type\":\"BillBreak\"').any(), axis=1)\n",
    "        ]\n",
    "        \n",
    "        # Usar expresiones regulares para extraer los valores\n",
    "        dfLogsbb['seqNumber'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"seqNumber\":\"(\\d+)\"').astype(str)\n",
    "        dfLogsbb['TicketData'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"TicketData\":\"([^\"]+)\"').astype(str)\n",
    "        dfLogsbb['Amount'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"Amount\":([0-9]+(?:\\.[0-9]+)?)')\n",
    "        dfLogsbb['DispensedTotal'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"DispensedTotal\":([0-9]+(?:\\.[0-9]+)?)')\n",
    "        #dfLogsTR['type'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"TransactionType\":\"([^\"]+)\"').astype(str)\n",
    "        #dfLogsTR['Type'] = dfLogsTR[\"FilteredData\"].str.extract(r'\"type\":\"([^\"]+)\"').astype(str)\n",
    "        dfLogsbb['Type'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"type\"\\s*:\\s*\"([^\"]+)\"', expand=False)        \n",
    "        dfLogsbb['BillDenom_01_1'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"BillCount_01\":([0-9]+(?:\\.[0-9]+)?)')        \n",
    "        dfLogsbb['BillDenom_02_5'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"BillCount_02\":([0-9]+(?:\\.[0-9]+)?)')        \n",
    "        dfLogsbb['BillDenom_03_10'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"BillCount_03\":([0-9]+(?:\\.[0-9]+)?)')        \n",
    "        dfLogsbb['BillDenom_04_20'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"BillCount_04\":([0-9]+(?:\\.[0-9]+)?)')        \n",
    "        dfLogsbb['BillDenom_05_50'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"BillCount_05\":([0-9]+(?:\\.[0-9]+)?)')\n",
    "        dfLogsbb['BillDenom_06_100'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"BillCount_06\":([0-9]+(?:\\.[0-9]+)?)')\n",
    "        dfLogsbb['Status'] = dfLogsbb[\"FilteredData\"].str.extract(r'\"Status\":\"([^\"]+)\"').astype(str)\n",
    "        dfLogsbb['JournalName'] = os.path.basename(ruta_subcarpeta).lower()\n",
    "\n",
    "print(dfLogsbb)\n",
    "\n",
    "#df_LogsUnified.to_excel(output_path, index=False, engine='openpyxl')  # Usa openpyxl como motor\n",
    "#print(f\"Archivo exportado exitosamente a {output_path}\")\n",
    "\n",
    "# Guardar el primer dataframe en la primera hoja\n",
    "with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "    df_LogsUnified.to_excel(writer, sheet_name='CARDLOGS', index=False)\n",
    "\n",
    "# Agregar el segundo dataframe en la segunda hoja\n",
    "with pd.ExcelWriter(output_path,mode='a',engine='openpyxl') as writer:\n",
    "    dfLogsTR.to_excel(writer, sheet_name='TRLOGS', index=False)\n",
    "\n",
    "with pd.ExcelWriter(output_path,mode='a',engine='openpyxl') as writer:\n",
    "    dfLogsbb.to_excel(writer, sheet_name='BBLOGS', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8209f72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo contiene 'Bill Breaking' en el nombre.\n",
      "Nombre del archivo: Bill_Breaking_Transaction-20240903-0921.xls\n",
      "Transformacion Nulos\n",
      "\n",
      "El archivo contiene 'Voucher Redemption' en el nombre.\n",
      "Nombre del archivo: Voucher_Redemption_Transaction-20240903-0928.xls\n",
      "Transformacion Nulos\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_120716\\3966079688.py:34: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_tr.fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------LiveOffice BB and TR Reports----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Obtener una lista de todos los archivos Excel en la carpeta\n",
    "file_list = [os.path.join(folder_path_tr_bb, f) for f in os.listdir(folder_path_tr_bb) if f.endswith(('.xls', '.xlsx'))]\n",
    "\n",
    "dataframes = []\n",
    "for file in file_list:\n",
    "    file_name = os.path.basename(file)\n",
    "    if \"Voucher_Redemption_Transaction\" in file_name:\n",
    "        print(\"El archivo contiene 'Voucher Redemption' en el nombre.\")\n",
    "        df_tr = pd.read_excel(file, skiprows=15, header=None)\n",
    "        new_columns = df_tr.iloc[0].astype(str) + df_tr.iloc[1].astype(str)\n",
    "        df_tr.columns = new_columns  # Asignar los nuevos encabezados\n",
    "        df_tr = df_tr[2:]  # Eliminar las filas de encabezado original (0, 1 y 2)\n",
    "        df_tr.reset_index(drop=True, inplace=True)\n",
    "        #print (df.head())\n",
    "        df_tr['file_name'] = os.path.basename(file)# Agregar el nombre del archivo    \n",
    "        print(\"Nombre del archivo:\", file_name)\n",
    "        #Tranformación 1 - Nulos\n",
    "        # Eliminar columnas donde todos los valores son NaN\n",
    "        print (\"Transformacion Nulos\\n\")\n",
    "        df_tr = df_tr.dropna(axis=1, how='all')\n",
    "        #Tranformación 3 - Reemplaza NaN con un valor predeterminado si es necesario\n",
    "        #df = df.fillna(0)    \n",
    "        # Encuentra el nombre exacto de la columna que contiene 'DATE & TIME'\n",
    "        target_column = df_tr.filter(like=\"DATE & TIME\").columns[0]\n",
    "        # Renombra dinámicamente la columna encontrada\n",
    "        df_tr = df_tr.rename(columns={target_column: \"DATE & TIME\"})\n",
    "        df_tr = df_tr.loc[df_tr[\"DATE & TIME\"] != \"Totals:\"]\n",
    "        #print(df) \n",
    "        columnas_a_convertir = [\"TRANSAMOUNT($)\", \"DISPENSED QTYS$1\", \"nan$5\", \"nan$10\", \"nan$20\", \"nan$50\", \"nan$100\"]\n",
    "        df_tr[columnas_a_convertir] = df_tr[columnas_a_convertir].astype(float)\n",
    "        # Rellenar valores NaN con un número válido (ejemplo: 0)\n",
    "        df_tr.fillna(0, inplace=True)\n",
    "    elif \"Bill_Breaking_Transaction\" in file_name:\n",
    "        print(\"El archivo contiene 'Bill Breaking' en el nombre.\")        \n",
    "        df_bb = pd.read_excel(file, sheet_name=\"Sheet2\", skiprows=3, header=None)\n",
    "        file_name = os.path.basename(file)\n",
    "        new_columns = df_bb.iloc[2].astype(str) + df_bb.iloc[3].astype(str)\n",
    "        df_bb.columns = new_columns  # Asignar los nuevos encabezados\n",
    "        df_bb = df_bb[4:]  # Eliminar las filas de encabezado original (0, 1 y 2)\n",
    "        df_bb.reset_index(drop=True, inplace=True)\n",
    "        #print (df.head())\n",
    "        df_bb['file_name'] = os.path.basename(file)# Agregar el nombre del archivo    \n",
    "        print(\"Nombre del archivo:\", file_name)\n",
    "        #Tranformación 1 - Nulos\n",
    "        # Eliminar columnas donde todos los valores son NaN\n",
    "        print (\"Transformacion Nulos\\n\")\n",
    "        df_bb = df_bb.dropna(axis=1, how='all')\n",
    "        #Tranformación 3 - Reemplaza NaN con un valor predeterminado si es necesario\n",
    "        #df = df.fillna(0)    \n",
    "        # Encuentra el nombre exacto de la columna que contiene 'DATE & TIME'\n",
    "        target_column = df_bb.filter(like=\"DATE & TIME\").columns[0]\n",
    "        # Renombra dinámicamente la columna encontrada\n",
    "        df_bb = df_bb.rename(columns={target_column: \"DATE & TIME\"})\n",
    "        df_bb = df_bb.loc[df_bb[\"DATE & TIME\"] != \"Totals:\"]\n",
    "        #print(df) \n",
    "    \n",
    "#df_bb.to_excel(output_path3, index=False, engine='openpyxl')  # Usa openpyxl como motor\n",
    "#print(f\"Archivo exportado exitosamente a {output_path3}\")\n",
    "\n",
    "# Guardar el primer dataframe en la primera hoja\n",
    "with pd.ExcelWriter(output_path3, engine='openpyxl') as writer:\n",
    "    df_tr.to_excel(writer, sheet_name='TRLO', index=False)\n",
    "\n",
    "# Agregar el segundo dataframe en la segunda hoja\n",
    "with pd.ExcelWriter(output_path3,mode='a',engine='openpyxl') as writer:\n",
    "    df_bb.to_excel(writer, sheet_name='BBLO', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "174c1377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(dfLogsTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6cce81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe de LiveOffice\n",
      "root\n",
      " |-- DATE & TIME: string (nullable = true)\n",
      " |-- SEQUENCENUMBER: string (nullable = true)\n",
      " |-- TRANSAMOUNT($): long (nullable = true)\n",
      " |-- DISPENSED QTYS$1: long (nullable = true)\n",
      " |-- nan$5: long (nullable = true)\n",
      " |-- nan$10: long (nullable = true)\n",
      " |-- nan$20: long (nullable = true)\n",
      " |-- nan$50: long (nullable = true)\n",
      " |-- nan$100: long (nullable = true)\n",
      " |-- DISPENSED($): long (nullable = true)\n",
      " |-- MANUALLYPAID ($): long (nullable = true)\n",
      " |-- nanSTATUS: string (nullable = true)\n",
      " |-- file_name: string (nullable = true)\n",
      "\n",
      "Dataframe de Logs\n",
      "root\n",
      " |-- FilteredData: string (nullable = true)\n",
      " |-- seqNumber: string (nullable = true)\n",
      " |-- TicketData: string (nullable = true)\n",
      " |-- Amount: string (nullable = true)\n",
      " |-- DispensedTotal: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- JournalName: string (nullable = true)\n",
      "\n",
      "+--------------------+----------------+--------------------+------+--------------+----------------+-----------------+----------------+-----------+--------------+------------+--------------+----------------+-----+------+------+------+-------+-------------+-----------------+---------+-------------+---------+---------+----------------+\n",
      "|        FilteredData|       seqNumber|          TicketData|Amount|DispensedTotal|            Type|           Status|     JournalName|DATE & TIME|SEQUENCENUMBER|VOUCHERSDATA|TRANSAMOUNT($)|DISPENSED QTYS$1|nan$5|nan$10|nan$20|nan$50|nan$100|COINSTOTAL($)|DISPENSEDTOTAL($)|UNPAID($)|MANUALPAY ($)|nanSTATUS|file_name|found_in_Systems|\n",
      "+--------------------+----------------+--------------------+------+--------------+----------------+-----------------+----------------+-----------+--------------+------------+--------------+----------------+-----+------+------+------+-------+-------------+-----------------+---------+-------------+---------+---------+----------------+\n",
      "|00:02:27 400024 '...|0823202400020208|05894012708064163...| 551.5|         551.0|TicketRedemption|PARTIALDISPENSING|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:02:37 400024 '...|0823202400020208|05894012708064163...| 551.5|         551.0|TicketRedemption|        COMPLETED|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:03:56 400024 '...|0823202400032730|05248197198448986...|   2.0|           2.0|TicketRedemption|        COMPLETED|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:06:08 400024 '...|0823202400052615|05786643949252013...|626.24|         626.0|TicketRedemption|PARTIALDISPENSING|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:06:17 400024 '...|0823202400052615|05786643949252013...|626.24|         626.0|TicketRedemption|        COMPLETED|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:10:42 400024 '...|0823202400101443|05834157661339729...| 500.0|         500.0|TicketRedemption|        COMPLETED|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:11:15 400024 '...|0823202400105403|05071588152020278...| 15.65|          15.0|TicketRedemption|PARTIALDISPENSING|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:11:25 400024 '...|0823202400105403|05071588152020278...| 15.65|          15.0|TicketRedemption|        COMPLETED|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:13:10 400024 '...|0823202400124297|05342020675821966...| 37.33|          37.0|TicketRedemption|PARTIALDISPENSING|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:13:20 400024 '...|0823202400124297|05342020675821966...| 37.33|          37.0|TicketRedemption|        COMPLETED|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:14:38 400024 '...|0823202400141450|05591316146696689...|130.14|         130.0|TicketRedemption|PARTIALDISPENSING|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:14:47 400024 '...|0823202400141450|05591316146696689...|130.14|         130.0|TicketRedemption|        COMPLETED|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:23:32 400024 '...|0823202400231013|05051654435626234...| 303.0|         303.0|TicketRedemption|        COMPLETED|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:24:24 400024 '...|0823202400235061|05292941300591543...| 484.4|         484.0|TicketRedemption|        COMPLETED|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:24:15 400024 '...|0823202400235061|05292941300591543...| 484.4|         484.0|TicketRedemption|PARTIALDISPENSING|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:31:09 400024 '...|0823202400304283|05736419912714537...|1209.0|        1209.0|TicketRedemption|        COMPLETED|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:34:38 400024 '...|0823202400341364|05176492634958001...| 140.0|         140.0|TicketRedemption|        COMPLETED|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:36:36 400024 '...|0823202400361134|05218588627563647...|180.11|         180.0|TicketRedemption|PARTIALDISPENSING|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:36:46 400024 '...|0823202400361134|05218588627563647...|180.11|         180.0|TicketRedemption|        COMPLETED|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "|00:50:40 400024 '...|0823202400500672|05707332921414566...|250.52|         250.0|TicketRedemption|        COMPLETED|ticketredemption|       NULL|          NULL|        NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|         NULL|             NULL|     NULL|         NULL|     NULL|     NULL|           false|\n",
      "+--------------------+----------------+--------------------+------+--------------+----------------+-----------------+----------------+-----------+--------------+------------+--------------+----------------+-----+------+------+------+-------+-------------+-----------------+---------+-------------+---------+---------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+----------------+----------------+------+--------------+---------+---------+------------+-----------+--------------+--------------+----------------+-----+------+------+------+-------+------------+----------------+---------+---------+-------------------+\n",
      "|        FilteredData|       seqNumber|      TicketData|Amount|DispensedTotal|     Type|   Status| JournalName|DATE & TIME|SEQUENCENUMBER|TRANSAMOUNT($)|DISPENSED QTYS$1|nan$5|nan$10|nan$20|nan$50|nan$100|DISPENSED($)|MANUALLYPAID ($)|nanSTATUS|file_name|found_in_datastream|\n",
      "+--------------------+----------------+----------------+------+--------------+---------+---------+------------+-----------+--------------+--------------+----------------+-----+------+------+------+-------+------------+----------------+---------+---------+-------------------+\n",
      "|00:00:31 402001 '...|0823202400000976| BB[2]     $20.0|  20.0|          20.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|00:01:07 402001 '...|0823202400004898|BB[2]     $100.0| 100.0|         100.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|00:16:35 402001 '...|0823202400162040|  BB[2]     $5.0|   5.0|           5.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|00:40:11 402001 '...|0823202400395160|BB[2]     $100.0| 100.0|         100.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|00:52:20 402001 '...|0823202400515980|BB[2]     $100.0| 100.0|         100.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|00:54:32 402001 '...|0823202400541068| BB[2]     $20.0|  20.0|          20.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|01:01:05 402001 '...|0823202401004576|BB[2]     $100.0| 100.0|         100.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|01:11:36 402001 '...|0823202401111607| BB[2]     $20.0|  20.0|          20.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|01:22:19 402001 '...|0823202401220090|BB[2]     $100.0| 100.0|         100.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|01:22:46 402001 '...|0823202401222800|BB[2]     $100.0| 100.0|         100.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|01:24:03 402001 '...|0823202401234722| BB[2]     $20.0|  20.0|          20.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|01:37:00 402001 '...|0823202401364083|BB[2]     $100.0| 100.0|         100.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|01:37:24 402001 '...|0823202401370972| BB[2]     $20.0|  20.0|          20.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|01:38:08 402001 '...|0823202401375075|  BB[2]     $5.0|   5.0|           5.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|01:38:36 402001 '...|0823202401381679|  BB[2]     $5.0|   5.0|           5.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|02:02:35 402001 '...|0823202402022061| BB[2]     $20.0|  20.0|          20.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|02:04:09 402001 '...|0823202402035021|BB[2]     $100.0| 100.0|         100.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|02:08:16 402001 '...|0823202402075653| BB[2]     $20.0|  20.0|          20.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|02:17:25 402001 '...|0823202402170692|BB[2]     $100.0| 100.0|         100.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "|02:17:59 402001 '...|0823202402174134|BB[2]     $100.0| 100.0|         100.0|BillBreak|COMPLETED|billbreaking|       NULL|          NULL|          NULL|            NULL| NULL|  NULL|  NULL|  NULL|   NULL|        NULL|            NULL|     NULL|     NULL|              false|\n",
      "+--------------------+----------------+----------------+------+--------------+---------+---------+------------+-----------+--------------+--------------+----------------+-----+------+------+------+-------+------------+----------------+---------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Archivo exportado exitosamente a C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Entrega3\\TESIS_Maestria_ETL\\ETLProyect\\ArchivosSalida\\Concatenado3.xlsx\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------SPARK Consolidado logs\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Logs_LO TR_BB Comparative\").getOrCreate()\n",
    "\n",
    "#Pandas a Spark\n",
    "#print(dfLogsTR.head())\n",
    "#dfLogsTR = dfLogsTR.toPandas()\n",
    "dfLOGSTR = spark.createDataFrame(dfLogsTR)\n",
    "dfLogsBB = spark.createDataFrame(dfLogsbb)\n",
    "\n",
    "\n",
    "#print (df_tr)\n",
    "dfLOTR = spark.createDataFrame(df_tr)\n",
    "dfLOBB = spark.createDataFrame(df_bb) \n",
    "\n",
    "\n",
    "#Renombre de columnas\n",
    "#dfLO = dfLO.withColumnRenamed(\"HOSTSEQ NUMBER\", \"hostseq_number\")\n",
    "#dfHOST = dfHOST.withColumnRenamed(\"Seq\", \"seq\")\n",
    "\n",
    "# Filtrar el DataFrame dfLO\n",
    "#dfLO = dfLO.filter(F.col(\"STATUS\") != \"Denied\")\n",
    "#dfLO = dfLO.filter(F.col(\"TRANSACTIONTYPE\") != \"Balance inquiry\")\n",
    "\n",
    "\n",
    "#Verificación\n",
    "print('Dataframe de LiveOffice')\n",
    "dfLOBB.printSchema()\n",
    "print('Dataframe de Logs')\n",
    "dfLogsBB.printSchema()\n",
    "\n",
    "#dfLOGSTR = dfLOGSTR.filter(F.col(\"SEQUENCENUMBER\") != \"\")\n",
    "\n",
    "\n",
    "#-Join Logs con LOHOSTUNIFIED\n",
    "joined_df1 = dfLOGSTR.join(\n",
    "    dfLOTR,\n",
    "    dfLOGSTR[\"seqNumber\"] == dfLOTR[\"SEQUENCENUMBER\"],\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "sorted_df1 = joined_df1.orderBy(\"seqNumber\")\n",
    "\n",
    "\n",
    "# Agregar columna de indicador\n",
    "\n",
    "result_df2 = sorted_df1.withColumn(\n",
    "    \"found_in_Systems\",\n",
    "    F.when(F.col(\"SEQUENCENUMBER\").isNotNull(), True).otherwise(False)\n",
    ")\n",
    "\n",
    "result_df2.show()\n",
    "\n",
    "#-Join Logs con HOST\n",
    "joined_df2 = dfLogsBB.join(\n",
    "    dfLOBB,\n",
    "    dfLogsBB[\"seqNumber\"] == dfLOBB[\"SEQUENCENUMBER\"],\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "sorted_df2 = joined_df2.orderBy(\"seqNumber\")\n",
    "\n",
    "# Agregar columna de indicador\n",
    "result_df3 = sorted_df2.withColumn(\n",
    "    \"found_in_datastream\",\n",
    "    F.when(F.col(\"SEQUENCENUMBER\").isNotNull(), True).otherwise(False)\n",
    ")\n",
    "\n",
    "result_df3.show()\n",
    "\n",
    "pandas_df1 = result_df2.toPandas()\n",
    "pandas_df2 = result_df3.toPandas()\n",
    "\n",
    "with pd.ExcelWriter(output_path3, engine='openpyxl') as writer:\n",
    "    pandas_df1.to_excel(writer, sheet_name='LogsTR_LO', index=False)  # Guardar df1 en 'Hoja1'\n",
    "    pandas_df2.to_excel(writer, sheet_name='LogsBB_LO', index=False)  # Guardar df2 en 'Hoja2'\n",
    "\n",
    "\n",
    "\n",
    "#pandas_df.to_excel(output_path3, index=False, engine='openpyxl')  # Usa openpyxl como motor\n",
    "print(f\"Archivo exportado exitosamente a {output_path3}\")\n",
    "\n",
    "# Detener sesión de Spark\n",
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
