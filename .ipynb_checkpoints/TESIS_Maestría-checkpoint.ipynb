{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94b79be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e844ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\paula\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\paula\\anaconda3\\lib\\site-packages (1.24.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\paula\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\paula\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\paula\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\paula\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c672a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall -y pyspark\n",
    "#pip uninstall -yspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0691c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\paula\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "#Importar librerias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b570e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = r'C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Nueva carpeta\\Files\\ATM_Transaction_Summary-20240903-0919.xls'\n",
    "\n",
    "# Especificar la ruta del archivo Excel\n",
    "output_path = r'C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Nueva carpeta\\Files\\Procesados\\Concatenado2.xlsx'\n",
    "\n",
    "# Especificar la ruta del archivo Excel\n",
    "output_path3 = r'C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Nueva carpeta\\Files\\Procesados\\Concatenado3.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b40b477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titulos de archivo fuente:\n",
      "      0                    1                 2      3           4   \\\n",
      "0    NaN                  NaN               NaN    NaN         NaN   \n",
      "1    NaN                  NaN               NaN    NaN         NaN   \n",
      "2    NaN                 CS25          SEQUENCE    NaN        HOST   \n",
      "3    NaN          DATE & TIME            NUMBER  CARD#  SEQ NUMBER   \n",
      "4    NaN  08/23/2024 08:13:19  2024082308124108   7816        1452   \n",
      "...   ..                  ...               ...    ...         ...   \n",
      "1199 NaN  08/31/2024 04:15:39  2024083104150508   7577        2874   \n",
      "1200 NaN  08/31/2024 04:45:25  2024083104445708   6160         NaN   \n",
      "1201 NaN  08/31/2024 04:48:35  2024083104480808   1284        2877   \n",
      "1202 NaN  08/31/2024 05:10:02  2024083105092708   5484        2878   \n",
      "1203 NaN              Totals:               NaN    NaN         NaN   \n",
      "\n",
      "              5   6   7          8               9    10   11   12   13    14  \\\n",
      "0            NaN NaN NaN        NaN             NaN  NaN  NaN  NaN  NaN   NaN   \n",
      "1            NaN NaN NaN        NaN             NaN  NaN  NaN  NaN  NaN   NaN   \n",
      "2          TRANS NaN NaN      TRANS  DISPENSED QTYS  NaN  NaN  NaN  NaN   NaN   \n",
      "3           TYPE NaN NaN  AMOUNT($)              $1   $5  $10  $20  $50  $100   \n",
      "4     Withdrawal NaN NaN        100               0    0    0    0    0     1   \n",
      "...          ...  ..  ..        ...             ...  ...  ...  ...  ...   ...   \n",
      "1199  Withdrawal NaN NaN         60               0    0    0    3    0     0   \n",
      "1200  Withdrawal NaN NaN        100               0    0    0    0    0     0   \n",
      "1201  Withdrawal NaN NaN         40               0    0    0    2    0     0   \n",
      "1202  Withdrawal NaN NaN        200               0    0    0    0    0     2   \n",
      "1203         NaN NaN NaN     323640             NaN  NaN  NaN  NaN  NaN   NaN   \n",
      "\n",
      "             15  16  17         18         19  \n",
      "0           NaN NaN NaN        NaN        NaN  \n",
      "1           NaN NaN NaN        NaN        NaN  \n",
      "2         COINS NaN NaN  DISPENSED        NaN  \n",
      "3      TOTAL($) NaN NaN   TOTAL($)     STATUS  \n",
      "4             0 NaN NaN        100  Completed  \n",
      "...         ...  ..  ..        ...        ...  \n",
      "1199          0 NaN NaN         60  Completed  \n",
      "1200          0 NaN NaN          0     Denied  \n",
      "1201          0 NaN NaN         40  Completed  \n",
      "1202          0 NaN NaN        200  Completed  \n",
      "1203  Completed NaN NaN     241360        NaN  \n",
      "\n",
      "[1204 rows x 20 columns]\n",
      "Después de la unificación:\n",
      "      nannan      CS25DATE & TIME    SEQUENCENUMBER nanCARD# HOSTSEQ NUMBER  \\\n",
      "0        NaN  08/23/2024 08:13:19  2024082308124108     7816           1452   \n",
      "1        NaN  08/23/2024 08:27:15  2024082308262908     9584           1453   \n",
      "2        NaN  08/23/2024 08:46:09  2024082308453408     4299           1454   \n",
      "3        NaN  08/23/2024 09:06:48  2024082309063308     2610            NaN   \n",
      "4        NaN  08/23/2024 09:08:19  2024082309074608     6718           1456   \n",
      "...      ...                  ...               ...      ...            ...   \n",
      "1195     NaN  08/31/2024 04:15:39  2024083104150508     7577           2874   \n",
      "1196     NaN  08/31/2024 04:45:25  2024083104445708     6160            NaN   \n",
      "1197     NaN  08/31/2024 04:48:35  2024083104480808     1284           2877   \n",
      "1198     NaN  08/31/2024 05:10:02  2024083105092708     5484           2878   \n",
      "1199     NaN              Totals:               NaN      NaN            NaN   \n",
      "\n",
      "       TRANSTYPE  nannan  nannan TRANSAMOUNT($) DISPENSED QTYS$1 nan$5 nan$10  \\\n",
      "0     Withdrawal     NaN     NaN            100                0     0      0   \n",
      "1     Withdrawal     NaN     NaN             60                0     0      0   \n",
      "2     Withdrawal     NaN     NaN            600                0     0      0   \n",
      "3     Withdrawal     NaN     NaN           1000                0     0      0   \n",
      "4     Withdrawal     NaN     NaN           1000                0     0      0   \n",
      "...          ...     ...     ...            ...              ...   ...    ...   \n",
      "1195  Withdrawal     NaN     NaN             60                0     0      0   \n",
      "1196  Withdrawal     NaN     NaN            100                0     0      0   \n",
      "1197  Withdrawal     NaN     NaN             40                0     0      0   \n",
      "1198  Withdrawal     NaN     NaN            200                0     0      0   \n",
      "1199         NaN     NaN     NaN         323640              NaN   NaN    NaN   \n",
      "\n",
      "     nan$20 nan$50 nan$100 COINSTOTAL($)  nannan  nannan DISPENSEDTOTAL($)  \\\n",
      "0         0      0       1             0     NaN     NaN               100   \n",
      "1         3      0       0             0     NaN     NaN                60   \n",
      "2         0      0       6             0     NaN     NaN               600   \n",
      "3         0      0       0             0     NaN     NaN                 0   \n",
      "4         0      0      10             0     NaN     NaN              1000   \n",
      "...     ...    ...     ...           ...     ...     ...               ...   \n",
      "1195      3      0       0             0     NaN     NaN                60   \n",
      "1196      0      0       0             0     NaN     NaN                 0   \n",
      "1197      2      0       0             0     NaN     NaN                40   \n",
      "1198      0      0       2             0     NaN     NaN               200   \n",
      "1199    NaN    NaN     NaN     Completed     NaN     NaN            241360   \n",
      "\n",
      "      nanSTATUS  \n",
      "0     Completed  \n",
      "1     Completed  \n",
      "2     Completed  \n",
      "3        Denied  \n",
      "4     Completed  \n",
      "...         ...  \n",
      "1195  Completed  \n",
      "1196     Denied  \n",
      "1197  Completed  \n",
      "1198  Completed  \n",
      "1199        NaN  \n",
      "\n",
      "[1200 rows x 20 columns]\n",
      "nannan               float64\n",
      "CS25DATE & TIME       object\n",
      "SEQUENCENUMBER        object\n",
      "nanCARD#              object\n",
      "HOSTSEQ NUMBER        object\n",
      "TRANSTYPE             object\n",
      "nannan               float64\n",
      "nannan               float64\n",
      "TRANSAMOUNT($)        object\n",
      "DISPENSED QTYS$1      object\n",
      "nan$5                 object\n",
      "nan$10                object\n",
      "nan$20                object\n",
      "nan$50                object\n",
      "nan$100               object\n",
      "COINSTOTAL($)         object\n",
      "nannan               float64\n",
      "nannan               float64\n",
      "DISPENSEDTOTAL($)     object\n",
      "nanSTATUS             object\n",
      "dtype: object\n",
      "Transformacion Nulos\n",
      "\n",
      "          CS25DATE & TIME    SEQUENCENUMBER nanCARD# HOSTSEQ NUMBER  \\\n",
      "0     08/23/2024 08:13:19  2024082308124108     7816           1452   \n",
      "1     08/23/2024 08:27:15  2024082308262908     9584           1453   \n",
      "2     08/23/2024 08:46:09  2024082308453408     4299           1454   \n",
      "3     08/23/2024 09:06:48  2024082309063308     2610            NaN   \n",
      "4     08/23/2024 09:08:19  2024082309074608     6718           1456   \n",
      "...                   ...               ...      ...            ...   \n",
      "1195  08/31/2024 04:15:39  2024083104150508     7577           2874   \n",
      "1196  08/31/2024 04:45:25  2024083104445708     6160            NaN   \n",
      "1197  08/31/2024 04:48:35  2024083104480808     1284           2877   \n",
      "1198  08/31/2024 05:10:02  2024083105092708     5484           2878   \n",
      "1199              Totals:               NaN      NaN            NaN   \n",
      "\n",
      "       TRANSTYPE TRANSAMOUNT($) DISPENSED QTYS$1 nan$5 nan$10 nan$20 nan$50  \\\n",
      "0     Withdrawal            100                0     0      0      0      0   \n",
      "1     Withdrawal             60                0     0      0      3      0   \n",
      "2     Withdrawal            600                0     0      0      0      0   \n",
      "3     Withdrawal           1000                0     0      0      0      0   \n",
      "4     Withdrawal           1000                0     0      0      0      0   \n",
      "...          ...            ...              ...   ...    ...    ...    ...   \n",
      "1195  Withdrawal             60                0     0      0      3      0   \n",
      "1196  Withdrawal            100                0     0      0      0      0   \n",
      "1197  Withdrawal             40                0     0      0      2      0   \n",
      "1198  Withdrawal            200                0     0      0      0      0   \n",
      "1199         NaN         323640              NaN   NaN    NaN    NaN    NaN   \n",
      "\n",
      "     nan$100 COINSTOTAL($) DISPENSEDTOTAL($)  nanSTATUS  \n",
      "0          1             0               100  Completed  \n",
      "1          0             0                60  Completed  \n",
      "2          6             0               600  Completed  \n",
      "3          0             0                 0     Denied  \n",
      "4         10             0              1000  Completed  \n",
      "...      ...           ...               ...        ...  \n",
      "1195       0             0                60  Completed  \n",
      "1196       0             0                 0     Denied  \n",
      "1197       0             0                40  Completed  \n",
      "1198       2             0               200  Completed  \n",
      "1199     NaN     Completed            241360        NaN  \n",
      "\n",
      "[1200 rows x 15 columns]\n",
      "          CS25DATE & TIME    SEQUENCENUMBER nanCARD# HOSTSEQ NUMBER  \\\n",
      "0     08/23/2024 08:13:19  2024082308124108     7816           1452   \n",
      "1     08/23/2024 08:27:15  2024082308262908     9584           1453   \n",
      "2     08/23/2024 08:46:09  2024082308453408     4299           1454   \n",
      "3     08/23/2024 09:06:48  2024082309063308     2610              0   \n",
      "4     08/23/2024 09:08:19  2024082309074608     6718           1456   \n",
      "...                   ...               ...      ...            ...   \n",
      "1195  08/31/2024 04:15:39  2024083104150508     7577           2874   \n",
      "1196  08/31/2024 04:45:25  2024083104445708     6160              0   \n",
      "1197  08/31/2024 04:48:35  2024083104480808     1284           2877   \n",
      "1198  08/31/2024 05:10:02  2024083105092708     5484           2878   \n",
      "1199              Totals:                 0        0              0   \n",
      "\n",
      "       TRANSTYPE  TRANSAMOUNT($)  DISPENSED QTYS$1  nan$5  nan$10  nan$20  \\\n",
      "0     Withdrawal           100.0                 0      0       0       0   \n",
      "1     Withdrawal            60.0                 0      0       0       3   \n",
      "2     Withdrawal           600.0                 0      0       0       0   \n",
      "3     Withdrawal          1000.0                 0      0       0       0   \n",
      "4     Withdrawal          1000.0                 0      0       0       0   \n",
      "...          ...             ...               ...    ...     ...     ...   \n",
      "1195  Withdrawal            60.0                 0      0       0       3   \n",
      "1196  Withdrawal           100.0                 0      0       0       0   \n",
      "1197  Withdrawal            40.0                 0      0       0       2   \n",
      "1198  Withdrawal           200.0                 0      0       0       0   \n",
      "1199           0        323640.0                 0      0       0       0   \n",
      "\n",
      "      nan$50  nan$100 COINSTOTAL($)  DISPENSEDTOTAL($)  nanSTATUS  \n",
      "0          0        1             0              100.0  Completed  \n",
      "1          0        0             0               60.0  Completed  \n",
      "2          0        6             0              600.0  Completed  \n",
      "3          0        0             0                0.0     Denied  \n",
      "4          0       10             0             1000.0  Completed  \n",
      "...      ...      ...           ...                ...        ...  \n",
      "1195       0        0             0               60.0  Completed  \n",
      "1196       0        0             0                0.0     Denied  \n",
      "1197       0        0             0               40.0  Completed  \n",
      "1198       0        2             0              200.0  Completed  \n",
      "1199       0        0     Completed           241360.0          0  \n",
      "\n",
      "[1200 rows x 15 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_4868\\792319441.py:33: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo exportado exitosamente a C:\\Users\\paula\\UNIR\\MasterBigDatayVisualAnalytics\\cuatrimestre2\\TFM-TFE\\Nueva carpeta\\Files\\Procesados\\Concatenado2.xlsx\n"
     ]
    }
   ],
   "source": [
    "#Código para concatenar filas 3 y 4 para los titulos \n",
    "\n",
    "df = pd.read_excel(file_path, sheet_name=\"Sheet2\", skiprows=3, header=None)\n",
    "\n",
    "#Antes de unificar\n",
    "print (\"Titulos de archivo fuente:\")\n",
    "print(df)\n",
    "\n",
    "#Combinar filas 3 y 4 \n",
    "\n",
    "new_columns = df.iloc[2].astype(str) + df.iloc[3].astype(str)\n",
    "df.columns = new_columns  # Asignar los nuevos encabezados\n",
    "df = df[4:]  # Eliminar las filas de encabezado original (0, 1 y 2)\n",
    "# Reiniciar el índice\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Después de la unificación:\")\n",
    "print(df)\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "#Tranformación 1 - Nulos\n",
    "# Eliminar columnas donde todos los valores son NaN\n",
    "print (\"Transformacion Nulos\\n\")\n",
    "df = df.dropna(axis=1, how='all')\n",
    "print(df) \n",
    "\n",
    "#Tranformación 2 - Limpia y Convertir las columnas numéricas a float\n",
    "df['DISPENSEDTOTAL($)'] = pd.to_numeric(df['DISPENSEDTOTAL($)'], errors='coerce').fillna(0).astype(float)\n",
    "df['TRANSAMOUNT($)'] = pd.to_numeric(df['TRANSAMOUNT($)'], errors='coerce').astype(float)\n",
    "\n",
    "#Tranformación 3 - Reemplaza NaN con un valor predeterminado si es necesario\n",
    "df = df.fillna(0)\n",
    "\n",
    "#Tranformación 4-  Convierte las columnas categóricas o de texto a tipo `str`\n",
    "df['TRANSTYPE'] = df['TRANSTYPE'].astype(str)\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n",
    "df = df.drop(['DISPENSED QTYS$1', 'nan$5', 'nan$10', 'nan$20', 'nan$50', 'nan$100'], axis=1)\n",
    "\n",
    "# Guardar el DataFrame de Pandas en Excel\n",
    "df.to_excel(output_path, index=False, engine='openpyxl')  # Usa openpyxl como motor\n",
    "print(f\"Archivo exportado exitosamente a {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf579abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CS25DATE & TIME: string (nullable = true)\n",
      " |-- SEQUENCENUMBER: string (nullable = true)\n",
      " |-- nanCARD#: string (nullable = true)\n",
      " |-- HOSTSEQ NUMBER: string (nullable = true)\n",
      " |-- TRANSTYPE: string (nullable = true)\n",
      " |-- TRANSAMOUNT($): double (nullable = true)\n",
      " |-- COINSTOTAL($): string (nullable = true)\n",
      " |-- DISPENSEDTOTAL($): double (nullable = true)\n",
      " |-- nanSTATUS: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o74.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (DESKTOP-NPCO3PU executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4324)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4868\\3542138459.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdfATM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdfATM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m         \"\"\"\n\u001b[1;32m--> 947\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m     def _show_string(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    966\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o74.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (DESKTOP-NPCO3PU executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4324)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n"
     ]
    }
   ],
   "source": [
    "#SPARK\n",
    "\n",
    "spark = SparkSession.builder.appName(\"LecturaArchivos\").getOrCreate()\n",
    "\n",
    "#Pandas a Spark\n",
    "\n",
    "dfATM = spark.createDataFrame(df)\n",
    "\n",
    "#Tranformación 1 - Nulos\n",
    "# Eliminar columnas donde todos los valores son NaN\n",
    "#print(\"Transformación Nulos\\n\")\n",
    "#dfATM = dfATM.select([col_name for col_name in dfATM.columns if not dfATM.filter(col(col_name).isNotNull()).count() == 0])\n",
    "\n",
    "dfATM.printSchema()\n",
    "dfATM.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7497f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación 2: Filtros\n",
    "# Filtro: DISPENSEDTOTAL($) > 0 y TRANSTYPE != 'Balance inquiry'\n",
    "dfATM_Completadas = dfATM.filter((dfATM['DISPENSEDTOTAL($)'] > 0) & (dfATM['TRANSTYPE'] != 'Balance inquiry'))\n",
    "\n",
    "\n",
    "\n",
    "dfATM_Completadas_pd = dfATM_Completadas.toPandas()\n",
    "\n",
    "# Guardar el DataFrame de Pandas en Excel\n",
    "dfATM_Completadas_pd.to_excel(output_path3, index=False, engine='openpyxl')  # Usa openpyxl como motor\n",
    "print(f\"Archivo exportado exitosamente a {output_path3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f08a5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
